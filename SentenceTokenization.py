from nltk.tokenize import word_tokenize

text = "This is a sample sentence for tokenization."
tokens = word_tokenize(text)
print(tokens)  # Output: ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenization', '.']
